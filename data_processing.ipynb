{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8262a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os, zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c1db65",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e02239eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3721\n",
      "1    2706\n",
      "7    2686\n",
      "4    1845\n",
      "3    1797\n",
      "5    1704\n",
      "2    1640\n",
      "6    1429\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label_raw</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>, ...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!</td>\n",
       "      <td>[1,  4,  7]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>... And I don't think we need to discuss the T...</td>\n",
       "      <td>[8,  1]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>* So get up out of your bed</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A confession that you hired [PERSON] ... and a...</td>\n",
       "      <td>[1,  6]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence    label_raw  label\n",
       "0                                              , ...          [1]      0\n",
       "1                                                  !  [1,  4,  7]      0\n",
       "2  ... And I don't think we need to discuss the T...      [8,  1]      7\n",
       "3                        * So get up out of your bed          [1]      0\n",
       "4  A confession that you hired [PERSON] ... and a...      [1,  6]      0"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data\n",
    "data = pd.read_csv('data/en-annotated.tsv', sep='\\t', header=None, names=['sentence', 'label_raw'])\n",
    "\n",
    "# get only the first label for now\n",
    "# unsure if that's the most \"important\" one or what to do later\n",
    "data['label_raw'] = data['label_raw'].str.split(',')\n",
    "data['label'] = pd.to_numeric(data.label_raw.str[0])-1  ## CNN seems to expect labels to start at 0\n",
    "\n",
    "# summarize first label\n",
    "print(data.label.value_counts())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b2693f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    21.228891\n",
       "1    15.438156\n",
       "7    15.324053\n",
       "4    10.526016\n",
       "3    10.252168\n",
       "5     9.721588\n",
       "2     9.356458\n",
       "6     8.152670\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of each label\n",
    "data.label.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8d4174f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape:  (11743,)\n",
      "Test dataset shape:  (5785,)\n"
     ]
    }
   ],
   "source": [
    "train_in, test_in, train_labels, test_labels = train_test_split(data['sentence'], data['label'], test_size = 0.33)\n",
    "print('Train dataset shape: ', train_in.shape)\n",
    "print('Test dataset shape: ', test_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "da5f7da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape:  (9394,)\n",
      "Dev dataset shape:  (2349,)\n"
     ]
    }
   ],
   "source": [
    "# In case we want a Dev set\n",
    "train_in, dev_in, train_labels, dev_labels = train_test_split(train_in, train_labels, test_size = 0.2)\n",
    "print('Train dataset shape: ', train_in.shape)\n",
    "print('Dev dataset shape: ', dev_in.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4620b398",
   "metadata": {},
   "source": [
    "### Baseline model - CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa354bf",
   "metadata": {},
   "source": [
    "#### tokenize and embed sentences\n",
    "- Using Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3f2b7d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length in train and test = 290\n"
     ]
    }
   ],
   "source": [
    "# Process sentences \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# tun into tokens\n",
    "# max len \n",
    "max_len = train_in.str.len().max()\n",
    "if test_in.str.len().max() > max_len: max_len = test_in.str.len().max()\n",
    "print('max sentence length in train and test =', max_len)\n",
    "\n",
    "# initialize tokenizer \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_in)\n",
    "\n",
    "# convert to sequences and pad\n",
    "train_sequences = tokenizer.texts_to_sequences(train_in)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_in)\n",
    "padding_type = \"post\"\n",
    "truncate_type = \"pre\"\n",
    "# use 100 for now\n",
    "max_len_touse = 100\n",
    "train_padded = pad_sequences(train_sequences,maxlen=max_len_touse, padding=padding_type, truncating=truncate_type)\n",
    "test_padded = pad_sequences(test_sequences,maxlen=max_len_touse, padding=padding_type, truncating=truncate_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c6b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download Glove model\n",
    "# based on https://cnvrg.io/cnn-sentence-classification/\n",
    "import wget\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "wget.download(url, out=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a1d349c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('data/glove.6B.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6d8cf8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('data/glove/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4a1d82cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, max_len_touse))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546fc4d6",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4939dd14",
   "metadata": {},
   "source": [
    "- First attempt: based on  https://cnvrg.io/cnn-sentence-classification/\n",
    "        Did not perform better than most common class (label 0 at 21.2%)... :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ee0d9f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kera embedding layer\n",
    "embedding_layer = keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "                            output_dim=max_len_touse,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_len_touse,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "bc16e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model - option 1 \n",
    "model_test = keras.models.Sequential([\n",
    "    embedding_layer,\n",
    "  keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "    keras.layers.GlobalMaxPooling1D(),\n",
    "  keras.layers.Dense(10, activation='relu'),\n",
    "  keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "093497fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "294/294 [==============================] - 7s 24ms/step - loss: 0.0000e+00 - accuracy: 0.1502\n",
      "Epoch 2/10\n",
      "294/294 [==============================] - 7s 24ms/step - loss: 0.0000e+00 - accuracy: 0.1531\n",
      "Epoch 3/10\n",
      "294/294 [==============================] - 7s 24ms/step - loss: 0.0000e+00 - accuracy: 0.1536\n",
      "Epoch 4/10\n",
      "294/294 [==============================] - 7s 24ms/step - loss: 0.0000e+00 - accuracy: 0.1531\n",
      "Epoch 5/10\n",
      "294/294 [==============================] - 7s 24ms/step - loss: 0.0000e+00 - accuracy: 0.1504\n",
      "Epoch 6/10\n",
      "294/294 [==============================] - 7s 24ms/step - loss: 0.0000e+00 - accuracy: 0.1502\n",
      "Epoch 7/10\n",
      "294/294 [==============================] - 7s 24ms/step - loss: 0.0000e+00 - accuracy: 0.1529\n",
      "Epoch 8/10\n",
      "294/294 [==============================] - 7s 24ms/step - loss: 0.0000e+00 - accuracy: 0.1512\n",
      "Epoch 9/10\n",
      "294/294 [==============================] - 7s 24ms/step - loss: 0.0000e+00 - accuracy: 0.1561\n",
      "Epoch 10/10\n",
      "294/294 [==============================] - 7s 24ms/step - loss: 0.0000e+00 - accuracy: 0.1511\n"
     ]
    }
   ],
   "source": [
    "# train model \n",
    "model_test.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "history = model_test.fit(train_padded, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b43a4",
   "metadata": {},
   "source": [
    "- Second attempt: based on CCN notebook from assignment 4\n",
    "        A little bit better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "823f8ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model - taken from CNN in A4\n",
    "epochs = 10\n",
    "embed_dim = 100\n",
    "num_filters = [2, 2, 2]\n",
    "kernel_sizes = [2, 3, 4]\n",
    "dense_layer_dims = [10, 4]\n",
    "dropout_rate = 0.7\n",
    "num_classes = 8\n",
    "\n",
    "# Input is a special \"layer\".  It defines a placeholder that will be overwritten by the training data.\n",
    "# In our case, we are accepting a list of wordids (padded out to max_len).\n",
    "wordids = keras.layers.Input(shape=(max_len_touse,))\n",
    "\n",
    "# Embed the wordids.\n",
    "# Recall, this is just a mathematically equivalent operation to a linear layer and a one-hot\n",
    "h = keras.layers.Embedding(len(tokenizer.word_index) + 1, embed_dim, input_length=max_len_touse)(wordids)\n",
    "\n",
    "# Construct \"filters\" randomly initialized filters with dimension \"kernel_size\" for each size of filter we want.\n",
    "# With the default hyperparameters, we construct 2 filters each of size 2, 3, 4.  As in the image above, each filter\n",
    "# is wide enough to span the whole word embedding (this is why the convolution is \"1d\" as seen in the\n",
    "# function name below).\n",
    "conv_layers_for_all_kernel_sizes = []\n",
    "for kernel_size, filters in zip(kernel_sizes, num_filters):\n",
    "    conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(h)\n",
    "    conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "    conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
    "\n",
    "# Concat the feature maps from each different size.\n",
    "h = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1)\n",
    "\n",
    "# Dropout can help with overfitting (improve generalization) by randomly 0-ing different subsets of values\n",
    "# in the vector.\n",
    "# See https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf for details.\n",
    "h = keras.layers.Dropout(rate=dropout_rate)(h)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "# Add a fully connected layer for each dense layer dimension in dense_layer_dims.\n",
    "dense_layers = []\n",
    "for dense_dim in dense_layer_dims:\n",
    "    dense_layer = keras.layers.Dense(dense_dim, activation='relu')(h)\n",
    "    dense_layers.append(dense_layer)\n",
    "    \n",
    "h = keras.layers.concatenate(dense_layers, axis=1)\n",
    "\n",
    "### END YOUR CODE\n",
    "\n",
    "prediction = keras.layers.Dense(num_classes, activation='softmax')(h)\n",
    "\n",
    "model = keras.Model(inputs=wordids, outputs=prediction)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # From information theory notebooks.\n",
    "              metrics=['accuracy'])        # What metric to output as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "efe85af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "294/294 [==============================] - 12s 37ms/step - loss: 2.0646 - accuracy: 0.1763\n",
      "Epoch 2/10\n",
      "294/294 [==============================] - 11s 37ms/step - loss: 2.0220 - accuracy: 0.2171\n",
      "Epoch 3/10\n",
      "294/294 [==============================] - 11s 37ms/step - loss: 1.9943 - accuracy: 0.2358\n",
      "Epoch 4/10\n",
      "294/294 [==============================] - 11s 37ms/step - loss: 1.9661 - accuracy: 0.2379\n",
      "Epoch 5/10\n",
      "294/294 [==============================] - 11s 37ms/step - loss: 1.9379 - accuracy: 0.2492\n",
      "Epoch 6/10\n",
      "294/294 [==============================] - 11s 37ms/step - loss: 1.9183 - accuracy: 0.2537\n",
      "Epoch 7/10\n",
      "294/294 [==============================] - 11s 37ms/step - loss: 1.8854 - accuracy: 0.2726\n",
      "Epoch 8/10\n",
      "294/294 [==============================] - 11s 37ms/step - loss: 1.8751 - accuracy: 0.2742\n",
      "Epoch 9/10\n",
      "294/294 [==============================] - 11s 37ms/step - loss: 1.8451 - accuracy: 0.2811\n",
      "Epoch 10/10\n",
      "294/294 [==============================] - 11s 37ms/step - loss: 1.8411 - accuracy: 0.2869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f71d5aa2e50>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.to_categorical(\n",
    "    train_labels, num_classes=None, dtype='float32'\n",
    ")\n",
    "model.fit(train_padded, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "77fb4d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181/181 [==============================] - 1s 5ms/step - loss: 1.9219 - accuracy: 0.2636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.92194402217865, 0.2636127769947052]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_padded, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "940e01ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 100, 100)     624700      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 99, 2)        402         embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 98, 2)        602         embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 97, 2)        802         embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 2)            0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_24 (Global (None, 2)            0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_25 (Global (None, 2)            0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 6)            0           global_max_pooling1d_23[0][0]    \n",
      "                                                                 global_max_pooling1d_24[0][0]    \n",
      "                                                                 global_max_pooling1d_25[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 6)            0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 10)           70          dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 4)            28          dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 14)           0           dense_33[0][0]                   \n",
      "                                                                 dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 8)            120         concatenate_7[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 626,724\n",
      "Trainable params: 626,724\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
