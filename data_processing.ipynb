{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8262a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os, zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c1db65",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e02239eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3721\n",
      "1    2706\n",
      "7    2686\n",
      "4    1845\n",
      "3    1797\n",
      "5    1704\n",
      "2    1640\n",
      "6    1429\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label_raw</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>, ...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!</td>\n",
       "      <td>[1,  4,  7]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>... And I don't think we need to discuss the T...</td>\n",
       "      <td>[8,  1]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>* So get up out of your bed</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A confession that you hired [PERSON] ... and a...</td>\n",
       "      <td>[1,  6]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence    label_raw  label\n",
       "0                                              , ...          [1]      0\n",
       "1                                                  !  [1,  4,  7]      0\n",
       "2  ... And I don't think we need to discuss the T...      [8,  1]      7\n",
       "3                        * So get up out of your bed          [1]      0\n",
       "4  A confession that you hired [PERSON] ... and a...      [1,  6]      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data\n",
    "data = pd.read_csv('data/en-annotated.tsv', sep='\\t', header=None, names=['sentence', 'label_raw'])\n",
    "\n",
    "# get only the first label for now\n",
    "# unsure if that's the most \"important\" one or what to do later\n",
    "data['label_raw'] = data['label_raw'].str.split(',')\n",
    "data['label'] = pd.to_numeric(data.label_raw.str[0])-1  ## CNN seems to expect labels to start at 0\n",
    "\n",
    "# summarize first label\n",
    "print(data.label.value_counts())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2693f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    21.228891\n",
       "1    15.438156\n",
       "7    15.324053\n",
       "4    10.526016\n",
       "3    10.252168\n",
       "5     9.721588\n",
       "2     9.356458\n",
       "6     8.152670\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of each label\n",
    "data.label.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d4174f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape:  (11743,)\n",
      "Test dataset shape:  (5785,)\n"
     ]
    }
   ],
   "source": [
    "train_in, test_in, train_labels, test_labels = train_test_split(data['sentence'], data['label'], test_size = 0.33)\n",
    "print('Train dataset shape: ', train_in.shape)\n",
    "print('Test dataset shape: ', test_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da5f7da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape:  (9394,)\n",
      "Dev dataset shape:  (2349,)\n"
     ]
    }
   ],
   "source": [
    "# In case we want a Dev set\n",
    "train_in, dev_in, train_labels, dev_labels = train_test_split(train_in, train_labels, test_size = 0.2)\n",
    "print('Train dataset shape: ', train_in.shape)\n",
    "print('Dev dataset shape: ', dev_in.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4620b398",
   "metadata": {},
   "source": [
    "### Baseline model - CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa354bf",
   "metadata": {},
   "source": [
    "#### tokenize and embed sentences\n",
    "- Using Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f2b7d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length in train and test = 299\n"
     ]
    }
   ],
   "source": [
    "# Process sentences \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# tun into tokens\n",
    "# max len \n",
    "max_len = train_in.str.len().max()\n",
    "if test_in.str.len().max() > max_len: max_len = test_in.str.len().max()\n",
    "print('max sentence length in train and test =', max_len)\n",
    "\n",
    "# initialize tokenizer \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_in)\n",
    "\n",
    "# convert to sequences and pad\n",
    "train_sequences = tokenizer.texts_to_sequences(train_in)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_in)\n",
    "padding_type = \"post\"\n",
    "truncate_type = \"pre\"\n",
    "# use 100 for now\n",
    "max_len_touse = 100\n",
    "train_padded = pad_sequences(train_sequences,maxlen=max_len_touse, padding=padding_type, truncating=truncate_type)\n",
    "test_padded = pad_sequences(test_sequences,maxlen=max_len_touse, padding=padding_type, truncating=truncate_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18c6b8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/glove.6B (1).zip'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download Glove model\n",
    "# based on https://cnvrg.io/cnn-sentence-classification/\n",
    "import wget\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "wget.download(url, out=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1d349c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('data/glove.6B.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d8cf8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('data/glove/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a1d82cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, max_len_touse))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546fc4d6",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4939dd14",
   "metadata": {},
   "source": [
    "- First attempt: based on  https://cnvrg.io/cnn-sentence-classification/\n",
    "        Did not perform better than most common class (label 0 at 21.2%)... :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee0d9f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kera embedding layer\n",
    "embedding_layer = keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "                            output_dim=max_len_touse,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_len_touse,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc16e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model - option 1 \n",
    "model_test = keras.models.Sequential([\n",
    "    embedding_layer,\n",
    "  keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "    keras.layers.GlobalMaxPooling1D(),\n",
    "  keras.layers.Dense(10, activation='relu'),\n",
    "  keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "093497fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "294/294 [==============================] - 2s 5ms/step - loss: 3.7240e-07 - accuracy: 0.2099\n",
      "Epoch 2/10\n",
      "294/294 [==============================] - 2s 6ms/step - loss: 3.7240e-07 - accuracy: 0.2099\n",
      "Epoch 3/10\n",
      "294/294 [==============================] - 1s 5ms/step - loss: 3.7240e-07 - accuracy: 0.2099\n",
      "Epoch 4/10\n",
      "294/294 [==============================] - 2s 6ms/step - loss: 3.7240e-07 - accuracy: 0.2099\n",
      "Epoch 5/10\n",
      "294/294 [==============================] - 2s 6ms/step - loss: 3.7240e-07 - accuracy: 0.2099\n",
      "Epoch 6/10\n",
      "294/294 [==============================] - 2s 6ms/step - loss: 3.7240e-07 - accuracy: 0.2099\n",
      "Epoch 7/10\n",
      "294/294 [==============================] - 2s 6ms/step - loss: 3.7240e-07 - accuracy: 0.2099\n",
      "Epoch 8/10\n",
      "294/294 [==============================] - 2s 6ms/step - loss: 3.7240e-07 - accuracy: 0.2099\n",
      "Epoch 9/10\n",
      "294/294 [==============================] - 2s 5ms/step - loss: 3.7240e-07 - accuracy: 0.2099\n",
      "Epoch 10/10\n",
      "294/294 [==============================] - 2s 6ms/step - loss: 3.7240e-07 - accuracy: 0.2099\n"
     ]
    }
   ],
   "source": [
    "# train model \n",
    "model_test.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "history = model_test.fit(train_padded, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b43a4",
   "metadata": {},
   "source": [
    "- Second attempt: based on CCN notebook from assignment 4\n",
    "        A little bit better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "823f8ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model - taken from CNN in A4\n",
    "epochs = 10\n",
    "embed_dim = 100\n",
    "num_filters = [2, 2, 2]\n",
    "kernel_sizes = [2, 3, 4]\n",
    "dense_layer_dims = [10, 4]\n",
    "dropout_rate = 0.7\n",
    "num_classes = 8\n",
    "\n",
    "# Input is a special \"layer\".  It defines a placeholder that will be overwritten by the training data.\n",
    "# In our case, we are accepting a list of wordids (padded out to max_len).\n",
    "wordids = keras.layers.Input(shape=(max_len_touse,))\n",
    "\n",
    "# Embed the wordids.\n",
    "# Recall, this is just a mathematically equivalent operation to a linear layer and a one-hot\n",
    "h = keras.layers.Embedding(len(tokenizer.word_index) + 1, embed_dim, input_length=max_len_touse)(wordids)\n",
    "\n",
    "# Construct \"filters\" randomly initialized filters with dimension \"kernel_size\" for each size of filter we want.\n",
    "# With the default hyperparameters, we construct 2 filters each of size 2, 3, 4.  As in the image above, each filter\n",
    "# is wide enough to span the whole word embedding (this is why the convolution is \"1d\" as seen in the\n",
    "# function name below).\n",
    "conv_layers_for_all_kernel_sizes = []\n",
    "for kernel_size, filters in zip(kernel_sizes, num_filters):\n",
    "    conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(h)\n",
    "    conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "    conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
    "\n",
    "# Concat the feature maps from each different size.\n",
    "h = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1)\n",
    "\n",
    "# Dropout can help with overfitting (improve generalization) by randomly 0-ing different subsets of values\n",
    "# in the vector.\n",
    "# See https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf for details.\n",
    "h = keras.layers.Dropout(rate=dropout_rate)(h)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "# Add a fully connected layer for each dense layer dimension in dense_layer_dims.\n",
    "dense_layers = []\n",
    "for dense_dim in dense_layer_dims:\n",
    "    dense_layer = keras.layers.Dense(dense_dim, activation='relu')(h)\n",
    "    dense_layers.append(dense_layer)\n",
    "    \n",
    "h = keras.layers.concatenate(dense_layers, axis=1)\n",
    "\n",
    "### END YOUR CODE\n",
    "\n",
    "prediction = keras.layers.Dense(num_classes, activation='softmax')(h)\n",
    "\n",
    "model = keras.Model(inputs=wordids, outputs=prediction)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # From information theory notebooks.\n",
    "              metrics=['accuracy'])        # What metric to output as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efe85af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "294/294 [==============================] - 4s 13ms/step - loss: 2.0538 - accuracy: 0.1931\n",
      "Epoch 2/10\n",
      "294/294 [==============================] - 4s 13ms/step - loss: 2.0185 - accuracy: 0.2128\n",
      "Epoch 3/10\n",
      "294/294 [==============================] - 4s 12ms/step - loss: 2.0005 - accuracy: 0.2212\n",
      "Epoch 4/10\n",
      "294/294 [==============================] - 4s 13ms/step - loss: 1.9816 - accuracy: 0.2243\n",
      "Epoch 5/10\n",
      "294/294 [==============================] - 4s 12ms/step - loss: 1.9540 - accuracy: 0.2340\n",
      "Epoch 6/10\n",
      "294/294 [==============================] - 4s 13ms/step - loss: 1.9346 - accuracy: 0.2440\n",
      "Epoch 7/10\n",
      "294/294 [==============================] - 4s 13ms/step - loss: 1.9118 - accuracy: 0.2527\n",
      "Epoch 8/10\n",
      "294/294 [==============================] - 4s 15ms/step - loss: 1.8885 - accuracy: 0.2544\n",
      "Epoch 9/10\n",
      "294/294 [==============================] - 4s 13ms/step - loss: 1.8799 - accuracy: 0.2532\n",
      "Epoch 10/10\n",
      "294/294 [==============================] - 4s 13ms/step - loss: 1.8538 - accuracy: 0.2656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14ab3c430>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.to_categorical(\n",
    "    train_labels, num_classes=None, dtype='float32'\n",
    ")\n",
    "model.fit(train_padded, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77fb4d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181/181 [==============================] - 0s 2ms/step - loss: 1.9494 - accuracy: 0.2553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9493663311004639, 0.25531548261642456]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_padded, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "940e01ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 100)     632700      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 99, 2)        402         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 98, 2)        602         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 97, 2)        802         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 2)            0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 2)            0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 2)            0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 6)            0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 6)            0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           70          dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            28          dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 14)           0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 8)            120         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 634,724\n",
      "Trainable params: 634,724\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79606507",
   "metadata": {},
   "source": [
    "## Training with Random Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee599a96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_sizes:  [5, 17, 2]\n",
      "num_filters:  [50, 34, 5]\n",
      "Epoch 1/10\n",
      "294/294 [==============================] - 8s 28ms/step - loss: 2.0355 - accuracy: 0.1872\n",
      "Epoch 2/10\n",
      "294/294 [==============================] - 8s 27ms/step - loss: 1.9491 - accuracy: 0.2427\n",
      "Epoch 3/10\n",
      "294/294 [==============================] - 8s 27ms/step - loss: 1.7995 - accuracy: 0.3288\n",
      "Epoch 4/10\n",
      "294/294 [==============================] - 8s 27ms/step - loss: 1.6120 - accuracy: 0.4212\n",
      "Epoch 5/10\n",
      "294/294 [==============================] - 8s 28ms/step - loss: 1.4194 - accuracy: 0.5045\n",
      "Epoch 6/10\n",
      "294/294 [==============================] - 9s 31ms/step - loss: 1.2464 - accuracy: 0.5813\n",
      "Epoch 7/10\n",
      "294/294 [==============================] - 9s 31ms/step - loss: 1.0972 - accuracy: 0.6320\n",
      "Epoch 8/10\n",
      "294/294 [==============================] - 9s 31ms/step - loss: 0.9923 - accuracy: 0.6646\n",
      "Epoch 9/10\n",
      "294/294 [==============================] - 11s 37ms/step - loss: 0.9161 - accuracy: 0.6872\n",
      "Epoch 10/10\n",
      "294/294 [==============================] - 10s 35ms/step - loss: 0.8510 - accuracy: 0.7092\n",
      "181/181 [==============================] - 1s 3ms/step - loss: 2.4435 - accuracy: 0.3238\n",
      "kernel_sizes:  [18, 12, 19]\n",
      "num_filters:  [36, 44, 14]\n",
      "Epoch 1/10\n",
      "294/294 [==============================] - 17s 57ms/step - loss: 2.0415 - accuracy: 0.1986\n",
      "Epoch 2/10\n",
      "294/294 [==============================] - 14s 47ms/step - loss: 1.9773 - accuracy: 0.2422\n",
      "Epoch 3/10\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 1.8087 - accuracy: 0.3361\n",
      "Epoch 4/10\n",
      "294/294 [==============================] - 14s 46ms/step - loss: 1.5819 - accuracy: 0.4337\n",
      "Epoch 5/10\n",
      "294/294 [==============================] - 14s 47ms/step - loss: 1.3727 - accuracy: 0.5281\n",
      "Epoch 6/10\n",
      "294/294 [==============================] - 15s 51ms/step - loss: 1.1850 - accuracy: 0.5988\n",
      "Epoch 7/10\n",
      "294/294 [==============================] - 17s 56ms/step - loss: 1.0377 - accuracy: 0.6570\n",
      "Epoch 8/10\n",
      "294/294 [==============================] - 19s 65ms/step - loss: 0.9081 - accuracy: 0.6986\n",
      "Epoch 9/10\n",
      "294/294 [==============================] - 18s 60ms/step - loss: 0.8136 - accuracy: 0.7273\n",
      "Epoch 10/10\n",
      "294/294 [==============================] - 16s 54ms/step - loss: 0.7403 - accuracy: 0.7549\n",
      "181/181 [==============================] - 1s 5ms/step - loss: 2.4417 - accuracy: 0.3366\n",
      "kernel_sizes:  [11, 4, 18]\n",
      "num_filters:  [44, 36, 10]\n",
      "Epoch 1/10\n",
      "294/294 [==============================] - 11s 36ms/step - loss: 2.0329 - accuracy: 0.2011\n",
      "Epoch 2/10\n",
      "294/294 [==============================] - 12s 42ms/step - loss: 1.9666 - accuracy: 0.2445\n",
      "Epoch 3/10\n",
      "294/294 [==============================] - 12s 39ms/step - loss: 1.8071 - accuracy: 0.3279\n",
      "Epoch 4/10\n",
      "294/294 [==============================] - 11s 38ms/step - loss: 1.6202 - accuracy: 0.4173\n",
      "Epoch 5/10\n",
      "294/294 [==============================] - 14s 46ms/step - loss: 1.4085 - accuracy: 0.5092\n",
      "Epoch 6/10\n",
      "294/294 [==============================] - 14s 47ms/step - loss: 1.2372 - accuracy: 0.5721\n",
      "Epoch 7/10\n",
      "294/294 [==============================] - 13s 43ms/step - loss: 1.0842 - accuracy: 0.6344\n",
      "Epoch 8/10\n",
      "294/294 [==============================] - 12s 42ms/step - loss: 0.9671 - accuracy: 0.6740\n",
      "Epoch 9/10\n",
      "294/294 [==============================] - 12s 40ms/step - loss: 0.8902 - accuracy: 0.6961\n",
      "Epoch 10/10\n",
      "294/294 [==============================] - 16s 53ms/step - loss: 0.8020 - accuracy: 0.7293\n",
      "181/181 [==============================] - 1s 5ms/step - loss: 2.4385 - accuracy: 0.3373\n",
      "kernel_sizes:  [12, 3, 10]\n",
      "num_filters:  [45, 12, 47]\n",
      "Epoch 1/10\n",
      "294/294 [==============================] - 13s 43ms/step - loss: 2.0351 - accuracy: 0.2055\n",
      "Epoch 2/10\n",
      "294/294 [==============================] - 14s 48ms/step - loss: 1.9399 - accuracy: 0.2543\n",
      "Epoch 3/10\n",
      "294/294 [==============================] - 14s 49ms/step - loss: 1.7503 - accuracy: 0.3576\n",
      "Epoch 4/10\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 1.5339 - accuracy: 0.4504\n",
      "Epoch 5/10\n",
      "294/294 [==============================] - 15s 52ms/step - loss: 1.3191 - accuracy: 0.5416\n",
      "Epoch 6/10\n",
      "294/294 [==============================] - 13s 44ms/step - loss: 1.1369 - accuracy: 0.6169\n",
      "Epoch 7/10\n",
      "294/294 [==============================] - 12s 41ms/step - loss: 0.9849 - accuracy: 0.6750\n",
      "Epoch 8/10\n",
      "294/294 [==============================] - 12s 41ms/step - loss: 0.8746 - accuracy: 0.7094\n",
      "Epoch 9/10\n",
      "294/294 [==============================] - 12s 41ms/step - loss: 0.7858 - accuracy: 0.7319\n",
      "Epoch 10/10\n",
      "294/294 [==============================] - 12s 41ms/step - loss: 0.7233 - accuracy: 0.7538\n",
      "181/181 [==============================] - 1s 4ms/step - loss: 2.5112 - accuracy: 0.3333\n",
      "kernel_sizes:  [7, 9, 17]\n",
      "num_filters:  [43, 2, 13]\n",
      "Epoch 1/10\n",
      "294/294 [==============================] - 14s 46ms/step - loss: 2.0490 - accuracy: 0.2058\n",
      "Epoch 2/10\n",
      "294/294 [==============================] - 14s 46ms/step - loss: 1.9830 - accuracy: 0.2333\n",
      "Epoch 3/10\n",
      "294/294 [==============================] - 14s 47ms/step - loss: 1.8590 - accuracy: 0.3116\n",
      "Epoch 4/10\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 1.6755 - accuracy: 0.3890\n",
      "Epoch 5/10\n",
      "294/294 [==============================] - 14s 49ms/step - loss: 1.5056 - accuracy: 0.4601\n",
      "Epoch 6/10\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 1.3637 - accuracy: 0.5204\n",
      "Epoch 7/10\n",
      "294/294 [==============================] - 14s 47ms/step - loss: 1.2402 - accuracy: 0.5681\n",
      "Epoch 8/10\n",
      "294/294 [==============================] - 15s 51ms/step - loss: 1.1300 - accuracy: 0.6042\n",
      "Epoch 9/10\n",
      "294/294 [==============================] - 15s 51ms/step - loss: 1.0531 - accuracy: 0.6358\n",
      "Epoch 10/10\n",
      "294/294 [==============================] - 14s 48ms/step - loss: 0.9774 - accuracy: 0.6561\n",
      "181/181 [==============================] - 1s 3ms/step - loss: 2.2310 - accuracy: 0.3277\n",
      "kernel_sizes:  [9, 20, 19]\n",
      "num_filters:  [41, 30, 16]\n",
      "Epoch 1/10\n",
      "294/294 [==============================] - 23s 78ms/step - loss: 2.0355 - accuracy: 0.2044\n",
      "Epoch 2/10\n",
      "294/294 [==============================] - 25s 87ms/step - loss: 1.9874 - accuracy: 0.2230\n",
      "Epoch 3/10\n",
      "294/294 [==============================] - 22s 76ms/step - loss: 1.8346 - accuracy: 0.3237\n",
      "Epoch 4/10\n",
      "294/294 [==============================] - 21s 70ms/step - loss: 1.5831 - accuracy: 0.4465\n",
      "Epoch 5/10\n",
      "294/294 [==============================] - 21s 70ms/step - loss: 1.3530 - accuracy: 0.5407\n",
      "Epoch 6/10\n",
      "294/294 [==============================] - 21s 70ms/step - loss: 1.1708 - accuracy: 0.6042\n",
      "Epoch 7/10\n",
      "294/294 [==============================] - 17s 59ms/step - loss: 1.0276 - accuracy: 0.6518\n",
      "Epoch 8/10\n",
      "294/294 [==============================] - 18s 62ms/step - loss: 0.9140 - accuracy: 0.6963\n",
      "Epoch 9/10\n",
      "294/294 [==============================] - 19s 65ms/step - loss: 0.8287 - accuracy: 0.7213\n",
      "Epoch 10/10\n",
      "294/294 [==============================] - 22s 76ms/step - loss: 0.7737 - accuracy: 0.7403\n",
      "181/181 [==============================] - 1s 6ms/step - loss: 2.5024 - accuracy: 0.3281\n",
      "kernel_sizes:  [10, 1, 5]\n",
      "num_filters:  [25, 4, 11]\n",
      "Epoch 1/10\n",
      "294/294 [==============================] - 12s 42ms/step - loss: 2.0397 - accuracy: 0.2029\n",
      "Epoch 2/10\n",
      "294/294 [==============================] - 12s 42ms/step - loss: 1.9877 - accuracy: 0.2239\n",
      "Epoch 3/10\n",
      "294/294 [==============================] - 10s 35ms/step - loss: 1.8808 - accuracy: 0.2890\n",
      "Epoch 4/10\n",
      "294/294 [==============================] - 9s 32ms/step - loss: 1.7349 - accuracy: 0.3486\n",
      "Epoch 5/10\n",
      "294/294 [==============================] - 10s 32ms/step - loss: 1.6015 - accuracy: 0.4076\n",
      "Epoch 6/10\n",
      "294/294 [==============================] - 9s 31ms/step - loss: 1.4883 - accuracy: 0.4474\n",
      "Epoch 7/10\n",
      "294/294 [==============================] - 10s 33ms/step - loss: 1.3972 - accuracy: 0.4808\n",
      "Epoch 8/10\n",
      "294/294 [==============================] - 10s 33ms/step - loss: 1.2895 - accuracy: 0.5243\n",
      "Epoch 9/10\n",
      "294/294 [==============================] - 10s 33ms/step - loss: 1.2236 - accuracy: 0.5468\n",
      "Epoch 10/10\n",
      "294/294 [==============================] - 9s 32ms/step - loss: 1.1479 - accuracy: 0.5765\n",
      "181/181 [==============================] - 1s 4ms/step - loss: 2.2480 - accuracy: 0.2989\n",
      "kernel_sizes:  [6, 15, 15]\n",
      "num_filters:  [31, 22, 29]\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294/294 [==============================] - 15s 50ms/step - loss: 2.0363 - accuracy: 0.1976\n",
      "Epoch 2/10\n",
      "294/294 [==============================] - 15s 52ms/step - loss: 1.9648 - accuracy: 0.2494\n",
      "Epoch 3/10\n",
      "294/294 [==============================] - 16s 53ms/step - loss: 1.7800 - accuracy: 0.3481\n",
      "Epoch 4/10\n",
      "294/294 [==============================] - 16s 54ms/step - loss: 1.5735 - accuracy: 0.4349\n",
      "Epoch 5/10\n",
      "294/294 [==============================] - 15s 53ms/step - loss: 1.3873 - accuracy: 0.5095\n",
      "Epoch 6/10\n",
      "294/294 [==============================] - 16s 54ms/step - loss: 1.2206 - accuracy: 0.5778\n",
      "Epoch 7/10\n",
      "294/294 [==============================] - 16s 56ms/step - loss: 1.0842 - accuracy: 0.6271\n",
      "Epoch 8/10\n",
      "294/294 [==============================] - 17s 59ms/step - loss: 0.9740 - accuracy: 0.6657\n",
      "Epoch 9/10\n",
      "294/294 [==============================] - 17s 58ms/step - loss: 0.8974 - accuracy: 0.6920\n",
      "Epoch 10/10\n",
      "294/294 [==============================] - 16s 54ms/step - loss: 0.8248 - accuracy: 0.7192\n",
      "181/181 [==============================] - 1s 5ms/step - loss: 2.4767 - accuracy: 0.3262\n",
      "kernel_sizes:  [13, 5, 2]\n",
      "num_filters:  [25, 11, 43]\n",
      "Epoch 1/10\n",
      "294/294 [==============================] - 12s 42ms/step - loss: 2.0430 - accuracy: 0.1995\n",
      "Epoch 2/10\n",
      "294/294 [==============================] - 12s 42ms/step - loss: 1.9689 - accuracy: 0.2444\n",
      "Epoch 3/10\n",
      "294/294 [==============================] - 13s 44ms/step - loss: 1.8053 - accuracy: 0.3267\n",
      "Epoch 4/10\n",
      "294/294 [==============================] - 12s 42ms/step - loss: 1.6338 - accuracy: 0.4003\n",
      "Epoch 5/10\n",
      "294/294 [==============================] - 12s 39ms/step - loss: 1.4760 - accuracy: 0.4644\n",
      "Epoch 6/10\n",
      "294/294 [==============================] - 13s 43ms/step - loss: 1.3354 - accuracy: 0.5403\n",
      "Epoch 7/10\n",
      "294/294 [==============================] - 14s 47ms/step - loss: 1.2045 - accuracy: 0.5857\n",
      "Epoch 8/10\n",
      "294/294 [==============================] - 13s 44ms/step - loss: 1.1079 - accuracy: 0.6174\n",
      "Epoch 9/10\n",
      "294/294 [==============================] - 14s 47ms/step - loss: 1.0310 - accuracy: 0.6468\n",
      "Epoch 10/10\n",
      "294/294 [==============================] - 15s 50ms/step - loss: 0.9542 - accuracy: 0.6727\n",
      "181/181 [==============================] - 1s 4ms/step - loss: 2.2850 - accuracy: 0.3421\n",
      "kernel_sizes:  [11, 15, 10]\n",
      "num_filters:  [47, 41, 46]\n",
      "Epoch 1/10\n",
      "294/294 [==============================] - 20s 69ms/step - loss: 2.0365 - accuracy: 0.2009\n",
      "Epoch 2/10\n",
      "294/294 [==============================] - 21s 71ms/step - loss: 1.9626 - accuracy: 0.2311\n",
      "Epoch 3/10\n",
      "294/294 [==============================] - 20s 68ms/step - loss: 1.7574 - accuracy: 0.3430\n",
      "Epoch 4/10\n",
      "294/294 [==============================] - 18s 62ms/step - loss: 1.5034 - accuracy: 0.4599\n",
      "Epoch 5/10\n",
      "294/294 [==============================] - 18s 62ms/step - loss: 1.2728 - accuracy: 0.5521\n",
      "Epoch 6/10\n",
      "294/294 [==============================] - 19s 65ms/step - loss: 1.0890 - accuracy: 0.6303\n",
      "Epoch 7/10\n",
      "294/294 [==============================] - 22s 74ms/step - loss: 0.9374 - accuracy: 0.6850\n",
      "Epoch 8/10\n",
      "294/294 [==============================] - 21s 70ms/step - loss: 0.8277 - accuracy: 0.7205\n",
      "Epoch 9/10\n",
      "294/294 [==============================] - 19s 64ms/step - loss: 0.7277 - accuracy: 0.7520\n",
      "Epoch 10/10\n",
      "294/294 [==============================] - 18s 60ms/step - loss: 0.6619 - accuracy: 0.7751\n",
      "181/181 [==============================] - 1s 5ms/step - loss: 2.6461 - accuracy: 0.3321\n"
     ]
    }
   ],
   "source": [
    "# Implement Random Search for parameters: num_filters, kernel_sizes\n",
    "import random as rn\n",
    "for _ in range(10):\n",
    "    # Specify model hyperparameters.\n",
    "    epochs = 10\n",
    "    embed_dim = 100\n",
    "    num_filters = [rn.randint(1, 50), rn.randint(1, 50), rn.randint(1, 50)]\n",
    "    kernel_sizes = [rn.randint(1, 20), rn.randint(1, 20), rn.randint(1, 20)]\n",
    "    dense_layer_dims = [10, 4]\n",
    "    dropout_rate = 0.7\n",
    "    num_classes = 8\n",
    "    print('kernel_sizes: ', kernel_sizes)\n",
    "    print('num_filters: ', num_filters)\n",
    "    # Construct the convolutional neural network.\n",
    "\n",
    "    # Input is a special \"layer\".  It defines a placeholder that will be overwritten by the training data.\n",
    "    # In our case, we are accepting a list of wordids (padded out to max_len).\n",
    "    wordids = keras.layers.Input(shape=(max_len_touse,))\n",
    "\n",
    "    # Embed the wordids.\n",
    "    # Recall, this is just a mathematically equivalent operation to a linear layer and a one-hot\n",
    "    h = keras.layers.Embedding(len(tokenizer.word_index) + 1, embed_dim, input_length=max_len_touse)(wordids)\n",
    "\n",
    "    # Construct \"filters\" randomly initialized filters with dimension \"kernel_size\" for each size of filter we want.\n",
    "    # With the default hyperparameters, we construct 2 filters each of size 2, 3, 4.  As in the image above, each filter\n",
    "    # is wide enough to span the whole word embedding (this is why the convolution is \"1d\" as seen in the\n",
    "    # function name below).\n",
    "    conv_layers_for_all_kernel_sizes = []\n",
    "    for kernel_size, filters in zip(kernel_sizes, num_filters):\n",
    "        conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(h)\n",
    "        conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "        conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
    "\n",
    "    # Concat the feature maps from each different size.\n",
    "    h = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1)\n",
    "\n",
    "    # Dropout can help with overfitting (improve generalization) by randomly 0-ing different subsets of values\n",
    "    # in the vector.\n",
    "    # See https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf for details.\n",
    "    h = keras.layers.Dropout(rate=dropout_rate)(h)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    # Add a fully connected layer for each dense layer dimension in dense_layer_dims.\n",
    "    dense_layers = []\n",
    "    for dense_dim in dense_layer_dims:\n",
    "        dense_layer = keras.layers.Dense(dense_dim, activation='relu')(h)\n",
    "        dense_layers.append(dense_layer)\n",
    "\n",
    "    h = keras.layers.concatenate(dense_layers, axis=1)\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    prediction = keras.layers.Dense(num_classes, activation='softmax')(h)\n",
    "\n",
    "    model = keras.Model(inputs=wordids, outputs=prediction)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',  # From information theory notebooks.\n",
    "                  metrics=['accuracy'])        # What metric to output as we train.\n",
    "    tf.keras.utils.to_categorical(\n",
    "        train_labels, num_classes=None, dtype='float32'\n",
    "    )\n",
    "    model.fit(train_padded, train_labels, epochs=epochs)\n",
    "    model.evaluate(test_padded, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24ce41b",
   "metadata": {},
   "source": [
    "## Train an RNN (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f15d1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
